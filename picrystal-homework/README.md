## Homework Tasks
- 1. Repeat (Run) the demo. (Send me the result json)
- 2. Assess the bias according to 3 feature: gender, race and citizenship
- 3. Assess the bias for department
- 4. The model returns probability of hiring. What if use threshold 0.7. Make comparisson assessment (in one run of picrystal)

## Homework Answer
- 1. I split the metrics_spec into several small metrics_spec files
- 2. `metrics_spec_bias_mvar.json` and `result_bias_mvar.json`
- 3. `metrics_spec_bias_department.json` and `result_bias_department.json`
- 4. `metrics_spec_performance.json` and `result_performance.json` (threshold 0.7 is not successful, see the question below)

## Questions and Suggestions for PiCrystal
- `BinaryEmbedderFromProbability` `@Denis` I added this embedder in the use case for task 4. However, an exception occurred, complaining more than 1 dimesion prediction. I printed out the predictions in the binary embedder class, and it does have two columns. I guess one for positive, one for negative. Is this a bug? Or did I configure the BinaryEmbedderFromProbability incorrectly in the use case?
``` [
 [False False]
 [ True False]
 [ True False]
 ...
]
(311, 2)
```
- `PiCrystal trace` `@Denis` Maybe we can consider adding a trace file of the use case execution. It could be helpful for users(data scientist or python developer) to understand the execution, also it may help them adjust or improve the use case. A trace can contain below information:
  - metric name
  - test function name
  - embedders (selected by which tag)
  - perturbers (selected by which tag)
  - execution time for each step
    - spent on the function
    - spent on embedders
    - spent on perturbers
    - spent on predictions
    - overhead if exists
  - resource consumption for each step (optional)
  - some specific information of this metric
  Moreover, the trace can be further enhanced to describe the DAG of test execution. It can even be visualized and added as a diagnostics tool in picrystal service. This is something like an "explain plan" tool in database that every DBA relies on. Also it could help promote and demo the product.
- `Support multiple tags in feature list` `@Denis` It seems that metric bias or bias_mvar does not support multiple tags in one feature list. I suppose the logic operation for tags is similar with SQL where condition:
```
feature_1: ["tag1" OR "tag2"] AND 
feature_2: ["tag3 OR tag4"] AND 
feature_3: [empty set represents any tag, i.e. all function-relevant embedders specified in the use case]
``` 
But it seems that each feature list can only have 0 or 1 tag. See an example in `metrics_spec_bias_problem.json`. Should we improve it? Or did I create the metrics spec incorrectly?
- `Regulation-specific metrics spec` `@Mike @Artur @Denis` Provide some metrics_spec templates based on regulation trust profiles. This step should also be integrated in the configuration workbench in PiCrystal service, so that it can be presented in key user dashboard. At first, the conversion from a trust profile to a metrics spec is a manual step. Later, it could be automated or at least standardized. Ideally, the trust profile and the metrics_spec of a certain AI regulation can be generated by an LLM, and verified by a regulation expert and then a data scientist.
